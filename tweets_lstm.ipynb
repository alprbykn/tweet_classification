{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweetlerin veritabanından alınması\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import zemberek\n",
    "import jpype,os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "durakwords= set(stopwords.words(\"turkish\"))\n",
    "import string\n",
    "import re\n",
    "from typing import List\n",
    "from jpype import JClass, JString, getDefaultJVMPath, shutdownJVM, startJVM, java\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,SpatialDropout1D,LSTM,Dense\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "%matplotlib inline\n",
    "df=pd.DataFrame()\n",
    "df=pd.read_csv(\"C:/Users/user/tweets/tweets2.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweetlerin küçük harfe dönüştürülmesi\n",
    "df= df.rename(columns = {'hash,content,class': 'tweets'})\n",
    "df['tweets']=df['tweets'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweetlerin son değerlerine göre yeni bir dataframe e yerleştirilmesi\n",
    "#1 tek,2 spor,eko,pol,mag,sağ,kült\n",
    "#df[df.tweets.str.endswith('2')]\n",
    "df['icerik']='icerik'\n",
    "df['cleantext']='cleantext'\n",
    "df.loc[df['tweets'].str.endswith('1'),'icerik']=\"teknoloji\"\n",
    "df.loc[df['tweets'].str.endswith('2'),'icerik']=\"spor\"\n",
    "df.loc[df['tweets'].str.endswith('3'),'icerik']=\"ekonomi\"\n",
    "df.loc[df['tweets'].str.endswith('4'),'icerik']=\"politika\"\n",
    "df.loc[df['tweets'].str.endswith('5'),'icerik']=\"magazin\"\n",
    "df.loc[df['tweets'].str.endswith('8'),'icerik']=\"kultur\"\n",
    "df.loc[df['tweets'].str.endswith('7'),'icerik']=\"saglik\"\n",
    "df.loc[df['tweets'].str.endswith('1'),'label']=\"1\"\n",
    "df.loc[df['tweets'].str.endswith('2'),'label']=\"2\"\n",
    "df.loc[df['tweets'].str.endswith('3'),'label']=\"3\"\n",
    "df.loc[df['tweets'].str.endswith('4'),'label']=\"4\"\n",
    "df.loc[df['tweets'].str.endswith('5'),'label']=\"5\"\n",
    "df.loc[df['tweets'].str.endswith('8'),'label']=\"8\"\n",
    "df.loc[df['tweets'].str.endswith('7'),'label']=\"7\"\n",
    "pd.to_numeric(df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zemberek kütüphanesinin kullanılabilmesi için Jpype ile JVM oluşturulması\n",
    "ZEMBEREK_PATH = r'C:\\Users\\user\\Desktop\\dersler\\tweets\\zemberek-full.jar'\n",
    "startJVM(getDefaultJVMPath(), '-ea', '-Djava.class.path=%s' % (ZEMBEREK_PATH))\n",
    "#Morphology kütüphanesini kullanacağımızdan burada oluşturuyoruz\n",
    "TurkishMorphology = JClass('zemberek.morphology.TurkishMorphology')\n",
    "morphology = TurkishMorphology.createWithDefaults()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mentionRemove(text):\n",
    "    text = re.sub(r'@\\w+', '', text)#mentionların çıkartırılması\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siteRemove(text):\n",
    "    text = re.sub(r'http.?://[^\\s]+[\\s]?', '', text)#bağlantıların çıkarılması\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def puncRemove(text):\n",
    "    text = re.sub('[^a-zığüşiöç\\s]', '', text)#özel karakterlerin ve sayıların çıkarılması\n",
    "    text = re.sub(r\"Â\", \"A\", text)#umlautlu karakterlerinn değiştirilmesi\n",
    "    text = re.sub(r\"â\", \"a\", text)\n",
    "    text = re.sub(r\"Î\", \"I\", text)\n",
    "    text = re.sub(r\"î\", \"ı\", text)\n",
    "    text = re.sub(r\"Û\", \"U\", text)\n",
    "    text = re.sub(r\"û\", \"u\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSw(text):\n",
    "    text=[w for w in tweet if w.lower() not in durakwords]#stopwordlerin çıkarılması\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rootFinder(text):\n",
    "    pos=[]\n",
    "    textlist=text.split(\" \")\n",
    "    analysis: java.util.ArrayList = ( morphology.analyzeAndDisambiguate(text).bestAnalysis() )\n",
    "    for i, analysis in enumerate(analysis, start=1):\n",
    "        f'\\nAnalysis {i}: {analysis}',\n",
    "        f'\\nPrimary POS {i}: {analysis.getPos()}' \n",
    "        if (str(analysis.getLemmas()[0]) == \"UNK\"):\n",
    "            pos.append(textlist[i-1])\n",
    "        else:     \n",
    "            pos.append(f'{str(analysis.getLemmas()[0])}')\n",
    "    return pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emojiRemove(tweet):\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' EMO_POS ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hash kısmının ve sondaki içerik belirteci ve tweet bağlantısının kaldırılması\n",
    "#utf-16 karakterlerin elenmesi\n",
    "#kelimelerin ayrıştırılması\n",
    "#tweet içerisindeki bağlantıların çıkarılması\n",
    "#tweet içindeki mentionların çıkarılması\n",
    "p=set(string.punctuation)\n",
    "\n",
    "def cleaner(text):\n",
    "    text=text\n",
    "    text=mentionRemove(text)#mentionların çıkartırılması\n",
    "    text=siteRemove(text)#bağlantıların çıkarılması\n",
    "    text=puncRemove(text)#özel karakterlerin ve sayıların çıkarılması\n",
    "    text=emojiRemove(text)\n",
    "    text=rootFinder(text)#kelime köklerinin bulunması\n",
    "    text=removeSw(text)#durak kelimelerin elenmesi\n",
    "    return text\n",
    "\n",
    "\n",
    "for x in range(0,len(df)):\n",
    "    #tweet=\"ve ya da için baba veya\"\n",
    "    tweet=df.iloc[x,0]\n",
    "    tweet=mentionRemove(tweet)#mentionların çıkartırılması\n",
    "    tweet=siteRemove(tweet)#bağlantıların çıkarılması\n",
    "    #tweet.replace(\"aracılığıyla\",\"a\")\n",
    "    tweet=puncRemove(tweet)#özel karakterlerin ve sayıların çıkarılması\n",
    "    tweet=rootFinder(tweet)#kelime köklerinin bulunması\n",
    "    tweet=removeSw(tweet)#durak kelimelerin elenmesi\n",
    "    tweet=' '.join(tweet)\n",
    "    #tweet=cleaner(tweet)\n",
    "    df.iloc[x,2]=tweet\n",
    "    pos=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "df.icerik.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter_words(df):\n",
    "    count = Counter()\n",
    "    for i in df:\n",
    "        for w in i.split():\n",
    "            count[w] += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=counter_words(df.cleantext)\n",
    "MAX_NB_WORDS = int(len(c)/2)\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 10\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_numeric(df.label)\n",
    "df[\"label\"] = df[\"label\"].astype(\"str\").astype(\"int\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(df.cleantext.values)\n",
    "word_index=tokenizer.word_index\n",
    "print(len(word_index))\n",
    "tweets=df.cleantext.values\n",
    "labels=df.icerik\n",
    "X=tokenizer.texts_to_sequences(tweets)#word_index\n",
    "X=pad_sequences(X,maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('dsa',X.shape)\n",
    "dummyY = pd.get_dummies(df.icerik)\n",
    "Y=dummyY.values\n",
    "print('Shape of label tensor:', Y.shape)\n",
    "#test_text=tokenizer.texts_to_sequences(test_text)\n",
    "#test_padded = pad_sequences(test_text,maxlen=max_len,padding = 'post',truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ycolumns=[]\n",
    "for i in dummyY.columns:\n",
    "    Ycolumns.append(i)\n",
    "Ycolumns\n",
    "print(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.3, random_state = 44)\n",
    "actual=Y_test\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)\n",
    "print(X_train[15])\n",
    "print(Y_train[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import regularizers\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1],embeddings_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "model.summary()\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,\n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_complaint = ['google mühendis ilk özellik']\n",
    "seq = tokenizer.texts_to_sequences(new_complaint)\n",
    "padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "pred = model.predict(padded)\n",
    "labels = ['ekonomi', 'kultur', 'magazin', 'politika', 'saglik', 'spor', 'teknoloji']\n",
    "print(pred, labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index=dict([(value,key) for (key,value) in word_index.items()])\n",
    "def decode(text):\n",
    "    return \" \".join([reverse_word_index.get(i,\"?\") for i in text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###predict a df item\n",
    "x=1#index 366 wrong-labeled\n",
    "item=X_test[x]\n",
    "item = [char for char in item]\n",
    "arr=np.array([])\n",
    "arr=arr.tolist()\n",
    "arr.append(item)\n",
    "arr.append(arr) \n",
    "arr=arr.pop()\n",
    "arr=np.array(arr)\n",
    "arr=pad_sequences(arr,maxlen=MAX_SEQUENCE_LENGTH)\n",
    "pred = model.predict(arr)\n",
    "labels = ['ekonomi', 'kultur', 'magazin', 'politika', 'saglik', 'spor', 'teknoloji']\n",
    "print(\"the input tweet is: \",decode(X_test[x]))\n",
    "print(pred, \"\\npredicted class is :\",labels[np.argmax(pred)])\n",
    "print(\"actual class is :\",labels[np.argmax(Y_test[x])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=model.predict(X_test).argmax(axis=1)\n",
    "le=LabelEncoder()\n",
    "pred=le.fit_transform(preds)\n",
    "#print(pred)\n",
    "pred=np.asarray(tf.keras.utils.to_categorical(pred))\n",
    "print(classification_report(pred.argmax(axis=1), actual.argmax(axis=1),target_names=['ekonomi', 'kultur', 'magazin', 'politika', 'saglik', 'spor', 'teknoloji']))\n",
    "print(\"accuracy score is\",accuracy_score(actual,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix( actual.argmax(axis=1),pred.argmax(axis=1))\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "sns.heatmap(conf_mat, annot=True, cmap=\"Blues\", fmt='d',\n",
    "            xticklabels={'ekonomi', 'kultur', 'magazin', 'politika', 'saglik', 'spor', 'teknoloji'},\n",
    "            yticklabels={'ekonomi', 'kultur', 'magazin', 'politika', 'saglik', 'spor', 'teknoloji'})\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title(\"CONFUSION MATRIX\\n\", size=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutdownJVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
