{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweetlerin veritabanından alınması\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import zemberek\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "durakwords= set(stopwords.words(\"turkish\"))\n",
    "import string\n",
    "import re\n",
    "import zemberek\n",
    "import jpype,os\n",
    "from typing import List\n",
    "from jpype import JClass, JString, getDefaultJVMPath, shutdownJVM, startJVM, java\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,SpatialDropout1D,LSTM,Dense\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "df=pd.DataFrame()\n",
    "df=pd.read_csv(\"C:/Users/user/tweets/tweets2.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zemberek kütüphanesinin kullanılabilmesi için Jpype ile JVM oluşturulması\n",
    "ZEMBEREK_PATH = r'C:\\Users\\user\\Desktop\\dersler\\tweets\\zemberek-full.jar'\n",
    "startJVM(getDefaultJVMPath(), '-ea', '-Djava.class.path=%s' % (ZEMBEREK_PATH))\n",
    "#Morphology kütüphanesini kullanacağımızdan burada oluşturuyoruz\n",
    "TurkishMorphology = JClass('zemberek.morphology.TurkishMorphology')\n",
    "morphology = TurkishMorphology.createWithDefaults()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweetlerin küçük harfe dönüştürülmesi\n",
    "df= df.rename(columns = {'hash,content,class': 'tweets'})\n",
    "df['tweets']=df['tweets'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweetlerin son değerlerine göre yeni bir dataframe e yerleştirilmesi\n",
    "#1 tek,2 spor,eko,pol,mag,sağ,kült\n",
    "#df[df.tweets.str.endswith('2')]\n",
    "df['icerik']='icerik'\n",
    "df['cleantext']='cleantext'\n",
    "df.loc[df['tweets'].str.endswith('1'),'icerik']=\"teknoloji\"\n",
    "df.loc[df['tweets'].str.endswith('2'),'icerik']=\"spor\"\n",
    "df.loc[df['tweets'].str.endswith('3'),'icerik']=\"ekonomi\"\n",
    "df.loc[df['tweets'].str.endswith('4'),'icerik']=\"politika\"\n",
    "df.loc[df['tweets'].str.endswith('5'),'icerik']=\"magazin\"\n",
    "df.loc[df['tweets'].str.endswith('8'),'icerik']=\"kultur\"\n",
    "df.loc[df['tweets'].str.endswith('7'),'icerik']=\"saglik\"\n",
    "df.loc[df['tweets'].str.endswith('1'),'label']=\"1\"\n",
    "df.loc[df['tweets'].str.endswith('2'),'label']=\"2\"\n",
    "df.loc[df['tweets'].str.endswith('3'),'label']=\"3\"\n",
    "df.loc[df['tweets'].str.endswith('4'),'label']=\"4\"\n",
    "df.loc[df['tweets'].str.endswith('5'),'label']=\"5\"\n",
    "df.loc[df['tweets'].str.endswith('8'),'label']=\"8\"\n",
    "df.loc[df['tweets'].str.endswith('7'),'label']=\"7\"\n",
    "pd.to_numeric(df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mentionRemove(text):\n",
    "    text = re.sub(r'@\\w+', '', text)#mentionların çıkartırılması\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siteRemove(text):\n",
    "    text = re.sub(r'http.?://[^\\s]+[\\s]?', '', text)#bağlantıların çıkarılması\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def puncRemove(text):\n",
    "    text = re.sub('[^a-zığüşiöç\\s]', '', text)#özel karakterlerin ve sayıların çıkarılması\n",
    "    text = re.sub(r\"Â\", \"A\", text)#umlautlu karakterlerinn değiştirilmesi\n",
    "    text = re.sub(r\"â\", \"a\", text)\n",
    "    text = re.sub(r\"Î\", \"I\", text)\n",
    "    text = re.sub(r\"î\", \"ı\", text)\n",
    "    text = re.sub(r\"Û\", \"U\", text)\n",
    "    text = re.sub(r\"û\", \"u\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSw(text):\n",
    "    text=[w for w in tweet if w.lower() not in durakwords]#stopwordlerin çıkarılması\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rootFinder(text):\n",
    "    pos=[]\n",
    "    textlist=text.split(\" \")\n",
    "    analysis: java.util.ArrayList = ( morphology.analyzeAndDisambiguate(text).bestAnalysis() )\n",
    "    for i, analysis in enumerate(analysis, start=1):\n",
    "        f'\\nAnalysis {i}: {analysis}',\n",
    "        f'\\nPrimary POS {i}: {analysis.getPos()}' \n",
    "        if (str(analysis.getLemmas()[0]) == \"UNK\"):\n",
    "            pos.append(textlist[i-1])\n",
    "        else:     \n",
    "            pos.append(f'{str(analysis.getLemmas()[0])}')\n",
    "    return pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emojiRemove(tweet):\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' EMO_POS ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hash kısmının ve sondaki içerik belirteci ve tweet bağlantısının kaldırılması\n",
    "#utf-16 karakterlerin elenmesi\n",
    "#kelimelerin ayrıştırılması\n",
    "#tweet içerisindeki bağlantıların çıkarılması\n",
    "#tweet içindeki mentionların çıkarılması\n",
    "p=set(string.punctuation)\n",
    "\n",
    "def cleaner(text):\n",
    "    text=text\n",
    "    text=mentionRemove(text)#mentionların çıkartırılması\n",
    "    text=siteRemove(text)#bağlantıların çıkarılması\n",
    "    text=puncRemove(text)#özel karakterlerin ve sayıların çıkarılması\n",
    "    text=emojiRemove(text)\n",
    "    text=rootFinder(text)#kelime köklerinin bulunması\n",
    "    text=removeSw(text)#durak kelimelerin elenmesi\n",
    "    return text\n",
    "for x in range(0,len(df)):\n",
    "    #tweet=\"ve ya da için baba veya\"\n",
    "    tweet=df.iloc[x,0]\n",
    "    tweet=mentionRemove(tweet)#mentionların çıkartırılması\n",
    "    tweet=siteRemove(tweet)#bağlantıların çıkarılması\n",
    "    #tweet.replace(\"aracılığıyla\",\"a\")\n",
    "    tweet=puncRemove(tweet)#özel karakterlerin ve sayıların çıkarılması\n",
    "    tweet=rootFinder(tweet)#kelime köklerinin bulunması\n",
    "    tweet=removeSw(tweet)#durak kelimelerin elenmesi\n",
    "    tweet=' '.join(tweet)\n",
    "    #tweet=cleaner(tweet)\n",
    "    df.iloc[x,2]=tweet\n",
    "    pos=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###KNN algorithm for k=27\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "texts=df.cleantext\n",
    "label=df.icerik\n",
    "train_text, test_text, train_label, test_label = train_test_split(texts,label,test_size=0.3,random_state=44)\n",
    "# vectorizing data\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_text = vectorizer.fit_transform(train_text)\n",
    "test_text = vectorizer.transform(test_text)\n",
    "# training\n",
    "classifier = KNeighborsClassifier(n_neighbors=27, weights='distance', algorithm='brute',\n",
    "                                  metric='euclidean')\n",
    "classifier.fit(train_text, train_label)\n",
    "# fitting\n",
    "predicted_labels = classifier.predict(test_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding Accuracy\n",
    "print(\"the accuracy score is \",accuracy_score(test_label,predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.predict(vectorizer.transform([\"aşı hekim\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_label, predicted_labels,target_names=['ekonomi', 'kultur', 'magazin', 'politika', 'saglik', 'spor', 'teknoloji']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(test_label, predicted_labels)\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "sns.heatmap(conf_mat, annot=True, cmap=\"Blues\", fmt='d',\n",
    "            xticklabels={'ekonomi', 'kultur', 'magazin', 'politika', 'saglik', 'spor', 'teknoloji'},\n",
    "            yticklabels={'ekonomi', 'kultur', 'magazin', 'politika', 'saglik', 'spor', 'teknoloji'})\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title(\"CONFUSION MATRIX\\n\", size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
